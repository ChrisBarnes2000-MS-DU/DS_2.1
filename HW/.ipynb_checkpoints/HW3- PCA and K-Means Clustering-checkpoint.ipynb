{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply PCA and Clustering to Wholesale Customer Data\n",
    "In this homework, we'll examine the [Wholesale Customers Dataset](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers), which we'll get from the UCI Machine Learning Datasets repository. This dataset contains the purchase records from clients of a wholesale distributor. It details the total annual purchases across categories seen in the data dictionary below:\n",
    "\n",
    "| Category | Description |\n",
    "|----------|-------------|\n",
    "CHANNEL\t|1= Hotel/Restaurant/Cafe, 2=Retailer (Nominal)|\n",
    "REGION\t|Geographic region of Portugal for each order (Nominal)|\n",
    "FRESH\t|Annual spending (m.u.) on fresh products (Continuous);|\n",
    "MILK\t|Annual spending (m.u.) on milk products (Continuous);|\n",
    "GROCERY\t|Annual spending (m.u.)on grocery products (Continuous);|\n",
    "FROZEN\t|Annual spending (m.u.)on frozen products (Continuous)|\n",
    "DETERGENTS_PAPER\t|Annual spending (m.u.) on detergents and paper products (Continuous)|\n",
    "DELICATESSEN\t|Annual spending (m.u.)on and delicatessen products (Continuous);|\n",
    "\n",
    "**TASK**: Read in wholesale_customers_data.csv from the datasets folder and store in a dataframe. Store the Channel column in a separate variable, and then drop the Channel and Region columns from the dataframe. Scale the data and use PCA to engineer new features (Principal Components). Print out the explained variance for each principal component.\n",
    "\n",
    "## K-Means, but Without All the Supervision\n",
    "**Challenge**: Use K-Means clustering on the wholesale_customers dataset, and then again on a version of this dataset transformed by PCA.\n",
    "\n",
    "1. Read in the data from the wholesale_customers_data.csv file contained within the datasets folder.\n",
    "\n",
    "2. Store the Channel column in a separate variable, and then drop the Region and Channel columns from the dataframe. Channel will act as our labels to tell us what class of customer each datapoint actually is, in case we want to check the accuracy of our clustering.\n",
    "\n",
    "3. Scale the data, fit a k-means object to it, and then visualize the data and the clustering.\n",
    "\n",
    "4. Use PCA to transform the data, and then use k-means clustering on it to see if our results are any better.\n",
    "\n",
    "**Challenge**: Use the confusion matrix function to create a confusion matrix and see how accurate our clustering algorithms were. Which did better--scaled data, or data transformed by PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in wholesale_customers_data.csv from the datasets folder and store in a dataframe. \n",
    "## 2. Store the Channel column in a separate variable, and then drop the Channel and Region columns from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Fresh   Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
      "0    12669   9656     7561     214              2674        1338\n",
      "1     7057   9810     9568    1762              3293        1776\n",
      "2     6353   8808     7684    2405              3516        7844\n",
      "3    13265   1196     4221    6404               507        1788\n",
      "4    22615   5410     7198    3915              1777        5185\n",
      "..     ...    ...      ...     ...               ...         ...\n",
      "435  29703  12051    16027   13135               182        2204\n",
      "436  39228   1431      764    4510                93        2346\n",
      "437  14531  15488    30243     437             14841        1867\n",
      "438  10290   1981     2232    1038               168        2125\n",
      "439   2787   1698     2510      65               477          52\n",
      "\n",
      "[440 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('wholesale_customers_data.csv')\n",
    "\n",
    "Channel = data['Channel']\n",
    "data = data.drop(columns=['Channel', 'Region'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scale the data and use PCA to engineer new features (Principal Components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   650.02212207   1585.51909007    -95.39064375   4540.78048148\n",
      "    -356.63711837    226.71184804]\n",
      " [ -4426.80497937   4042.45150884   1534.80474393   2567.65565913\n",
      "     -44.39428259    468.93801652]\n",
      " [ -4841.9987068    2578.762176     3801.38479014   2273.49433697\n",
      "    5245.3854378   -2141.12332875]\n",
      " ...\n",
      " [  4555.11499863  26201.75860287  -5887.43291863  -2082.90687562\n",
      "     -29.79580385  -1030.68216765]\n",
      " [ -2734.37092005  -7070.77533531   -790.70302471   1344.54788768\n",
      "    1448.4127231    -219.12615672]\n",
      " [-10370.12531409  -6161.46490876  -1017.14238084   1283.65788577\n",
      "     -80.6767127     297.82017043]]\n"
     ]
    }
   ],
   "source": [
    "# PCA computation by sklearn\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "# Find the principle components of 17 features\n",
    "X_r = pca.fit_transform(data)\n",
    "\n",
    "print(X_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Print out the explained variance for each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.64995904e+08 1.45452098e+08 2.51399785e+07 1.58039005e+07\n",
      " 5.39276364e+06 2.20364065e+06]\n",
      "--\n",
      "[0.45961362 0.40517227 0.07003008 0.04402344 0.01502212 0.00613848]\n",
      "--\n",
      "[0.45961362 0.86478588 0.93481597 0.97883941 0.99386152 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# print(X_r)\n",
    "# print(\"--\")\n",
    "print(pca.explained_variance_)\n",
    "print(\"--\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"--\")\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate the correlation of the principle components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of PCA Component:\n",
      "(2.0816681711721685e-17, 1.0000000000001332)\n"
     ]
    }
   ],
   "source": [
    "print('Correlation of PCA Component:')\n",
    "print(scipy.stats.pearsonr(X_r[:, 0], X_r[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
